{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frozen Lake - Value Iteration - RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solving Frozen Lake Problem using the value iteration.  DP and MDP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Isaias\\Anaconda2\\envs\\py\\lib\\site-packages\\gym\\envs\\registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the environmental variables of FrozenLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The number of states\n",
    "nS = env.env.nS\n",
    "nS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The number of actions\n",
    "nA = env.env.nA\n",
    "nA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.3333333333333333, 3, 0.0, False),\n",
       " (0.3333333333333333, 2, 0.0, False),\n",
       " (0.3333333333333333, 1, 0.0, False)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the initial [Transition_Probability, Next_State, Reward_Probability, flag]\n",
    "env.env.P[2][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.85"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration Function\n",
    "\n",
    "To solve the MDP via DP, first we initialize the value table with random numbers, then for each episode in the state we inspect every possible action and the sum is appended to a 'Q(s,a)' table, at the end, the best value of each action is stored in the value table and we stop when the difference of the real value and calculated values of the value table are not as different as the calcualted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iter(env, gamma=1.0, nepisodes=10000, eps=1e-20):\n",
    "    env.env.reset()\n",
    "    v_table = np.zeros(nS)\n",
    "    for i in range(nepisodes):\n",
    "        v_new = v_table.copy()\n",
    "        for s in range(nS):\n",
    "            q_val = []\n",
    "            for a in range(nA):\n",
    "                next_state_rewards = []\n",
    "                for T_prob, next_state, reward, _ in env.env.P[s][a]:\n",
    "                    next_state_rewards.append(T_prob*(reward + gamma*v_new[next_state]))\n",
    "                q_val.append(np.sum(next_state_rewards))\n",
    "                v_table[s] = max(q_val)\n",
    "        if (np.sum(abs(v_new - v_table)) <= eps):\n",
    "            print ('Stopping in %d episodes' %(i+1))\n",
    "            break\n",
    "    return v_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get The Optimal Values\n",
    "\n",
    "The optimal values in a future will serve us to extract the optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping in 197 episodes\n"
     ]
    }
   ],
   "source": [
    "optimal_value = value_iter(env=env, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03115544 0.02939836 0.04320513 0.02824951 0.0476495  0.\n",
      " 0.0798852  0.         0.08936976 0.17840342 0.23874263 0.\n",
      " 0.         0.30154673 0.58433243 0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(optimal_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the Optimal Policy\n",
    "\n",
    "When we have the optimal value we now can compute the optimal policy.  For extracting evaluate each action using optimal values and it will give us the optimal policy for each state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_policy(env, value_table, gamma=gamma):\n",
    "    policy = np.zeros(env.env.nS) \n",
    "    for state in range(env.env.nS):\n",
    "        Q_table = np.zeros(env.env.nA)\n",
    "        for action in range(env.env.nA):\n",
    "            for next_sr in env.env.P[state][action]: \n",
    "                trans_prob, next_state, reward_prob, _ = next_sr \n",
    "                Q_table[action] += (trans_prob * (reward_prob + gamma * value_table[next_state]))\n",
    "        policy[state] = np.argmax(Q_table)\n",
    "    \n",
    "    return policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_policy = extract_policy(env=env, value_table=optimal_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 3. 0. 3. 0. 0. 0. 0. 3. 1. 0. 0. 0. 2. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(optimal_policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
